# -*- coding: utf-8 -*-
"""11_19201128_Asif_Hasan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UyoL5Fky9K4EqSSJB2zkCWX12GC_N2oU

## CSE 422 Introduction to Data Preprocessing
---

### What are the advantages of preprocessing the data before applying on machine learning algorithm?

"The biggest advantage of pre-processing in ML is to improve **generalizablity** of your model. Data for any ML application is collected through some ‘sensors’. These sensors can be physical devices, instruments, software programs such as web crawlers, manual surveys, etc. Due to hardware malfunctions, software glitches, instrument failures, amd human errors, noise and erroneous information may creep in that can severely affect the performance of your model. Apart from **noise**, there are several **redundant information** that needs to be removed. For e.g. while predicting whether it rains tomorrow or not, age of the person is irrelevant. In terms of text processing, there are several stop words that may be redundant for the analysis. Lastly, there may be several **outliers** present in your data, due to the way data is collected that may need to be removed to improve the performance of the classifiers." 
                                    
                                            -Asif Hasan Chowdhury, CSE422, BRACU

1. Finding my dataset above

2. Load the dataset as dataframe using pandas

3. Handle missing values if needed

5. Encode categorical features is not needed because I don't have Any data that supports Categorical Encoding

6. Scale all the values between 0-1 with proper scaling technique

7. Split the dataset into features and labels. Use your intuition to determine which column indicates the labels.

Some Data Preprocessing Techniques:

* Imputation for missing values
* Handling Categorical Features
* Feature Normalization/Scaling
"""

#importing necessary libraries
import pandas as pd
import numpy as np

"""
#Load the dataset as dataframe using pandas

"""

volunteer = pd.read_csv('/content/sample_data/leaf_dataset.csv')
volunteer.head(4)

"""#Check datatype

"""

type(volunteer)

"""#Check List Shape 

"""

volunteer.shape

"""#Checking Dataset Null Values"""

volunteer.isnull().sum()

volunteer[['Elongation','Maximal Indentation Depth','Lobedness','Average Contrast']]

"""#Imputing Null Coloums                            
Elongation,Maximal Indentation Depth,Lobedness,Average Contrast

#Imputing Elongation
"""

from sklearn.impute import SimpleImputer

impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(volunteer[['Elongation']])

volunteer['Elongation'] = impute.transform(volunteer[['Elongation']])

"""#Imputing Maximal Indentation Depth"""

impute.fit(volunteer[['Maximal Indentation Depth']])

volunteer['Maximal Indentation Depth'] = impute.transform(volunteer[['Maximal Indentation Depth']])

"""#Imputing Lobedness"""

impute.fit(volunteer[['Lobedness']])

volunteer['Lobedness'] = impute.transform(volunteer[['Lobedness']])

"""#Imputing Average Contrast"""

impute.fit(volunteer[['Average Contrast']])

volunteer['Average Contrast'] = impute.transform(volunteer[['Average Contrast']])

"""#Checking for null values After imputing the Columns"""

volunteer.isnull().sum()

"""## Standardizing Data

## Feature Scaling

**MinMax Scaler:** 

Scales values to a range between 0 and 1 if no negative values, and -1 to 1 if there are negative values present.

$$\frac{X - X_{min}}{X_{max} - X_{min}}$$

where, 

 $$X\space is\space a\space feature\space value.$$ 
 $$X_{min} \space and \space X_{max} \space are \space corresponding \space feature's \space min \space and \space max \space values. $$

Sklearn library provides functions for different scalers by which we can easily scale our data.

#Finding Label Column
"""

y=volunteer.iloc[:,0:2]
y

specify_y=volunteer[['Class(species)']]
specify_y

"""#Finding Features Column"""

x=volunteer.iloc[:,2:]
x

"""#Using MinMaxScaler to Scale Features

**Scale all the values between 0-1 with proper scaling technique**
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(x)

# transform data
X_train_scaled = scaler.transform(x)

"""#Checking Scaled Datas"""

type(X_train_scaled)

"""#Checking Max & Min Value After Scaling

#Train Data & Max & Min Value
"""

X_train_scaled.max()

X_train_scaled.min()

"""#Datas After Scalling"""

X_train_scaled

"""#Spliting Scaled Data With implementing Stratification"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,specify_y, test_size = 0.25,random_state=0,stratify=specify_y)

"""#Check Train Dataset"""

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

"""#Check Test Dataset"""

print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""We can see that after Min-Max Scaling all the values are in the range [0,1]"""

print("per-feature minimum before scaling:\n {}".format(x.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

print("per-feature minimum after scaling:\n {}".format(
    X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(
    X_train_scaled.max(axis=0)))

# transform test data
X_test_scaled = scaler.transform(X_test)

"""## Effect of using MinMax Scaler:

### Accuracy with scaling

### We can see that accuracy improves if we train on scaled data.
"""

from sklearn.neighbors import KNeighborsClassifier
#X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,specify_y, test_size = 0.25,random_state=0,stratify=specify_y)
knn=KNeighborsClassifier()
y_train
knn.fit(X_train, y_train)
print(knn.score(X_test, y_test))
print("Test set accuracy: {:.2f}".format(knn.score(X_test, y_test)))

"""### Checking for correlated features

We may use the following heatmap to find out the correlation between each of the features in a dataset. If a certain feature is highly correlated with more than one feature, we may choose to drop that feature (in this case it is *flavanoids*) because it will affect our model in a similar way as the other two features (and thus will prove to redundant). Correlation between two features may be found using the color gradient shown on the right.
"""

wine = volunteer
wine.head()

wine_corr = wine.corr()
wine_corr

import seaborn as sns

sns.heatmap(wine_corr, cmap = 'YlGnBu')

"""---
.

**Reference**


* Müller Andreas Christian, and Sarah Guido. Introduction to Machine Learning with Python a Guide for Data Scientists. OReilly, 2018.

* DataCamp Python Course
"""